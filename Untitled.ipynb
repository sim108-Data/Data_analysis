{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction usefull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer les data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'Data/'\n",
    "\n",
    "HAPPINESS_DATASET = DATA_FOLDER+\"happiness2020.pkl\"\n",
    "COUNTRIES_DATASET = DATA_FOLDER+\"countries_info.csv\"\n",
    "\n",
    "happiness = pd.read_pickle(HAPPINESS_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 ) Creation d'un nouveau dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 ) Treatement of text **Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jungle\n"
     ]
    }
   ],
   "source": [
    "txt = \"welcome to the jungle\"\n",
    "\n",
    "x = txt.split()\n",
    "\n",
    "print(x[-1]) # ressort le premier pour trier par exemple des index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Poser des condition pour trier des dates par exemple rapidement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = (df_topics[\"index\"] >= pd.to_datetime(\"9th of March 2020\") - timedelta(days=34)) &\\\n",
    "        (df_topics[\"index\"] <= pd.to_datetime(\"9th of March 2020\"))\n",
    "cond2 = (weekly.index >= pd.to_datetime(\"01-01-19\")) & (weekly.index < pd.to_datetime(\"31-07-19\"))\n",
    "\n",
    "df_topics.loc[cond1, \"period\"] = \"before\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Groupby "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spent = np.zeros([4,3])\n",
    "type_=df.groupby(['Vehicle_type'])\n",
    "j=0\n",
    "print('\\n Time spent for each type of vehicle in each mode \\n')\n",
    "for veh_ty,veh_ty_df in type_:\n",
    "    i=0\n",
    "    tot=df.groupby(['Vehicle_type']).count()['Time']\n",
    "    print(veh_ty,'\\n')\n",
    "    id_ = veh_ty_df.groupby(['Mode'])\n",
    "    for mode , mode_df in id_:\n",
    "        print(mode,' : ',np.round(mode_df['Time'].count()/tot[j] *100,2))\n",
    "        count_mode = mode_df['Time'].count()\n",
    "        time_spent[i,j] = time_spent[i,j] + count_mode\n",
    "        i=i+1\n",
    "    j=j+1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time spent for each type of vehicle in each mode \n",
    "\n",
    "**Car** \n",
    "\n",
    "Accelerating  :  19.23 \n",
    "\n",
    "Cruising  :  20.53\n",
    "\n",
    "Decelerating  :  20.01\n",
    "\n",
    "Idling  :  40.24\n",
    "\n",
    "\n",
    "**Heavy Vehicle** \n",
    "\n",
    "Accelerating  :  16.17\n",
    "\n",
    "Cruising  :  21.59\n",
    "\n",
    "Decelerating  :  16.91\n",
    "\n",
    "Idling  :  45.33\n",
    "\n",
    "\n",
    "**Motorcycle** \n",
    "\n",
    "Accelerating  :  22.37\n",
    "\n",
    "Cruising  :  20.0\n",
    "\n",
    "Decelerating  :  24.29\n",
    "\n",
    "Idling  :  33.33\n",
    "\n",
    "Après ici on peut extraire ce quon veut dans un nouveau dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Groupby + agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step5=step5.groupby(['Vehicle_ID']).agg({'time_int': 'sum','distance_int':'sum',\n",
    "                                         'Speed':'mean','acceleration':'mean','Co2':'mean',\n",
    "                                         'is_aggressive': ['sum', 'count'] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)  **Melt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2015</th>\n",
       "      <th>2020</th>\n",
       "      <th>2025</th>\n",
       "      <th>2030</th>\n",
       "      <th>2035</th>\n",
       "      <th>2040</th>\n",
       "      <th>2045</th>\n",
       "      <th>2050</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2015  2020  2025  2030  2035  2040  2045  2050\n",
       "0     3     1     0     2     0     3     2     3\n",
       "1     2     1     2     1     3     2     1     1\n",
       "2     2     2     0     3     3     0     3     2\n",
       "3     1     0     1     3     2     1     1     2\n",
       "4     3     3     3     2     3     0     0     2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    str(i): np.random.randint(0, 4, 5) for i in np.arange(2015, 2055, 5)})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variable  value\n",
       "0     2015      3\n",
       "1     2015      2\n",
       "2     2015      2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melting = df.melt(ignore_index=False)\n",
    "melting.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Attention a bien formuler le **Apply** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics_filtered[\"view_percentage\"] = df_topics_filtered.apply(lambda x: x[\"value\"]/views[x[\"date\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Date data set  **Grouper** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedataset.groupby(pd.Grouper(freq=\"W\")).sum() # or mean, or max or etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_week=date_transpose.groupby(pd.Grouper(key='Date',freq='1W', origin='2020-01-01 00:00:00')).sum().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Fonction filter pour avoir seulement les index avec discussion dedans et les enlever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_filtered = articles.set_index(\"index\").filter(regex=r'^(?!Discussione:)', axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autre façon de faire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.loc[articles_df.index.str.contains('Discussione')]\n",
    "articles_df.drop(articles_df.loc[articles_df.index.str.contains('Discussione')== True].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sting to int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['authors_citations'] = df['authors_citations'].str.split(';').apply(lambda x: list(map(int,x)))\n",
    "df['authors_publications'] = df['authors_publications'].str.split(';').apply(lambda x: list(map(int,x)))\n",
    "df['authors_hindex'] = df['authors_hindex'].str.split(';').apply(lambda x: list(map(int,x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joindre des string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Facebook|Google|Microsoft|Deepmind'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_COMPANY = '|'.join([\"Facebook\", \"Google\", \"Microsoft\", \"Deepmind\"])\n",
    "TOP_COMPANY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) How to merge? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friend_house_location = houses.reset_index().rename({'user':'friend_edge'},axis = 'columns')\n",
    "    friend_house_location = network_df.merge(friend_house_location,\n",
    "                                           on = 'friend_edge').rename(\n",
    "        {'user_house_lat' : 'friend_house_lat','user_house_long' : 'friend_house_long'},\n",
    "        axis = \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_features=pd.concat([happiness,countries], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 ) print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} - {} ({})\".format(row.world_region, row.country, row.happiness_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. vertical line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axvline(pd.to_datetime(\"9 March 2020\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. axe y double "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.twiny()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. graph tous beau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [9, 6]\n",
    "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "sns.lineplot(x=df12_3[\"Cheminée d'équilibre\"][:181],y=df12_3['Niveau (m)'][:181],color='g')\n",
    "sns.lineplot(x=df12_3[\"Cheminée d'équilibre stabilisée à 12.3m3/s\"][:181],y=df12_3['Niveau (m) stabilisé à 12.3m3/s'][:181],color='g',ls='--')\n",
    "#plt.annotate('[1254 [m]]',color='b', xy=df.iloc[10,:], xytext=(df.iloc[10,0]+30,df.iloc[10,1]),arrowprops={'facecolor':'b', 'shrink':0.1} )\n",
    "bbox = dict(boxstyle=\"round\", facecolor='darkorange', alpha=0.5)\n",
    "arrowprops = dict(\n",
    "    color='darkorange',\n",
    "    arrowstyle = \"->\")\n",
    "    #connectionstyle = \"angle,angleA=0,angleB=-90,rad=10\")\n",
    "\n",
    "offset = 72\n",
    "plt.annotate('Min = (t={0:.02f}[s],Niveau:{1:.02f}[m])'.format(df12_3.iloc[10,0], df12_3.iloc[10,1]),\n",
    "            (df12_3.iloc[10,0], df12_3.iloc[10,1]), xytext=(0.5*offset, 0), textcoords='offset points',\n",
    "            bbox=bbox, arrowprops=arrowprops,fontsize=10)\n",
    "plt.annotate('Max = (t={0:.02f}[s],Niveau:{1:.02f}[m])'.format(max12_3.iloc[0,0],max12_3.iloc[0,1]),\n",
    "            (max12_3.iloc[0,0],max12_3.iloc[0,1]), xytext=(0.5*offset, 0), textcoords='offset points',\n",
    "            bbox=bbox, arrowprops=arrowprops,fontsize=10)\n",
    "plt.axhline(y=1275,xmin=0,xmax=3500, ls='--', lw=2,color='k');\n",
    "plt.legend(['Ouverture/fermeture à 12.3 m3/s','Ouverture puis stabilisation à 12.3 m3/s','Niveau moyen'],loc=(0.45,0.2))\n",
    "plt.suptitle(\"Niveau d'eau dans la chambre d'équilibre\",fontweight='bold')\n",
    "plt.xlabel('Temps [s]',fontweight = 'bold')\n",
    "plt.ylabel('Niveau [m]',fontweight = 'bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 comparer une variable binaire sur une colonne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = sns.FacetGrid(balanced_df_all, col='arxiv',margin_titles=True, height=5)\n",
    "g.map(plt.hist, 'reputation')\n",
    "plt.suptitle('Reputation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. add text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.text(accept_mean+0.1, 200,\n",
    "         'Mean Rating Acc.',rotation=90, color = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning\n",
    "from sklearn import model_selection, linear_model, metrics, ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Splitting data between train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,test_size = test_size, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "    reg_random = linear_model.LinearRegression().fit(X_train,y_train)\n",
    "    coef = reg_random.coef_\n",
    "    intercept = reg_random.intercept_\n",
    "    pred = reg_random.predict(X_test) #prediction\n",
    "    r2 = metrics.r2_score(y_test,pred) # score R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Définir un bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrap_CI(df, features, max_iter, drop_interval_size):\n",
    "    '''\n",
    "    Input : full dataset df , features used for regression , number of iteration max_iter\n",
    "    Output : confidence intervals using the empirical distribution with [2.5%,97.5%] of data.\n",
    "    '''\n",
    "    y = df['ratings']\n",
    "    X = df.drop('ratings',axis=1)\n",
    "    X = X[features]\n",
    "    ci = []\n",
    "    for i in range(max_iter):\n",
    "        X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size=.5,random_state=None)\n",
    "        reg_random = linear_model.LinearRegression().fit(X_train,y_train)\n",
    "        pred = reg_random.predict(X_test)\n",
    "\n",
    "        residual = y_test - pred\n",
    "        ci.append(((residual > 2).sum() + (residual < -2).sum()) / residual.count())\n",
    "    \n",
    "    confidence_interval = sorted(ci)\n",
    "    \n",
    "    return [confidence_interval[drop_interval_size],confidence_interval[-drop_interval_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Définir une hot vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['arxiv'], prefix = 'arxiv')\n",
    "one_hot = pd.get_dummies(yt,columns=['channel'],prefix=['channel_'])\n",
    "df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Grandient boosting regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boost_regression(df, features):\n",
    "    '''\n",
    "    Input: full dataset df , features used for regression\n",
    "    Output: y_test of our test df, our prediction 'pred' and R2 score\n",
    "    '''\n",
    "    \n",
    "    y = df['ratings']\n",
    "    X = df.drop('ratings',axis=1)\n",
    "    X = df[features]\n",
    "    #splitting\n",
    "    X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size = .3)\n",
    "    \n",
    "    #Gradient Boosting Regression\n",
    "    reg_random = ensemble.GradientBoostingRegressor().fit(X_train,y_train)\n",
    "    pred = reg_random.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test,pred)\n",
    "    \n",
    "    return y_test, pred, r2, reg_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. build N_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_N_fold(X, N_fold, seed = 1):\n",
    "    '''\n",
    "    Input: dataset X , prediction feature y , number of folds N_fold\n",
    "    Output: array with index representing the N-folds\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed) # Initializing the random \n",
    "    permut = np.random.permutation(X.shape[0]) #  permutation of the indexes of the dataset\n",
    "    indices_N = np.split(permut[:N_fold*int(X.shape[0]/N_fold)],N_fold) # Split the maximal part of the dataset that \n",
    "    # can be divided by N_fold into N_fold mini dataset\n",
    "    return indices_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross validation\n",
    "y = df['ratings']\n",
    "X = df.drop('ratings',axis=1)\n",
    "X = df[NEW_FEATURES]\n",
    "n_estimators=75\n",
    "learning_rate=0.05\n",
    "R2=[]\n",
    "X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size = .3,random_state=1 )\n",
    "N_fold_index_df = build_N_fold(X_train, N_fold=20, seed = 1)\n",
    "\n",
    "# crossvalidation\n",
    "for fold in  N_fold_index_df:\n",
    "    X_test_1fold = X_train.iloc[fold]\n",
    "    X_train_19fold = X_train.drop(X_train.iloc[fold].index)\n",
    "    y_test_1fold = y_train.iloc[fold]\n",
    "    y_train_19fold = y_train.drop(y_train.iloc[fold].index)\n",
    "    gb = ensemble.GradientBoostingRegressor(n_estimators=n_estimators,learning_rate=learning_rate)\n",
    "    .fit(X_train_19fold,y_train_19fold)\n",
    "    pred = gb.predict(X_test_1fold)\n",
    "    r2 = metrics.r2_score(y_test_1fold,pred)\n",
    "    R2.append(r2)\n",
    "print('R2 mean for the parameters (n_estimators = {0} , learning_rate = {1:.02f}) is {2:.04f}.'\n",
    "      .format(n_estimators,learning_rate,np.mean(R2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Sklearn cross valdation gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of regressor\n",
    "gb = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "# settings\n",
    "parameters = {'n_estimators': [50, 75, 100, 150, 200, 250],\n",
    "              'learning_rate': [.1, .05, .01]}\n",
    "y = df['ratings']\n",
    "X = df.drop('ratings',axis=1)\n",
    "X = df[NEW_FEATURES]\n",
    "\n",
    "# splitting the train and test dataset\n",
    "X_train, X_test, y_train, y_test = split_data_randomly(X,y,test_size = .3, random_state = 1 )\n",
    "\n",
    "# cross k validation using GridsearchCV\n",
    "clf = model_selection.GridSearchCV(gb, parameters, cv = 20, scoring = 'r2')\n",
    "clf.fit(X_train,y_train)\n",
    "best_param = clf.best_params_\n",
    "print('The best hyper parameters found with cross k validation with a k=20 are {}.'.format(best_param ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to have the result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "# extract all k=20 step\n",
    "result = result.iloc[:,result.columns.str.contains('split')]\n",
    "result.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) T-test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accept_2020' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6725a1b88bc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mttest_ind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccept_2020\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreject_2020\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'accept_2020' is not defined"
     ]
    }
   ],
   "source": [
    "scipy.stats.ttest_ind(accept_2020, reject_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) k-test --> tester si notre distribution vient d'un expenentielle : exp ou normale  :norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic.kstest_normal(df['IncomePerCap'].values, dist = 'exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examining relationship between two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson is most appropriate for measurements taken from an interval scale (temperature, dates, lengths, etc), while the Spearman is best for measurements taken from ordinal scales (rank orders, spectrum of values (agree, neutral, disagree), or healthy vs non-healthy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(df['IncomePerCap'],df['Employed']) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(df['IncomePerCap'],df['Employed']) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Logit regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a logistic regression on the X and y:\n",
    "\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logit_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a92f19f6d3ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mchance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mchance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rating\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logit_model' is not defined"
     ]
    }
   ],
   "source": [
    "# show the logit on a cool graph\n",
    "\n",
    "ratings = np.linspace(0, 9, 100, endpoint=True)\n",
    "chance = np.zeros(ratings.size)\n",
    "for i, rate in enumerate(ratings):\n",
    "    chance[i] = logit_model.predict([1, rate])[0]\n",
    "plt.plot(ratings, chance)\n",
    "plt.xlabel(\"rating\")\n",
    "plt.ylabel(\"Acceptance chance\")\n",
    "plt.scatter(X.ratings, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'decisions ~ ratings + reputation + C(arxiv_True)'\n",
    "logit_model_2 = sm.Logit.from_formula(formula, data = df_20).fit()\n",
    "logit_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'decisions ~ ratings + reputation + C(arxiv_True) + C(has_top_institution) + arxiv_True:has_top_institution'\n",
    "logit_model_3 = sm.Logit.from_formula(formula,data = df_20).fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adaexam]",
   "language": "python",
   "name": "conda-env-adaexam-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
